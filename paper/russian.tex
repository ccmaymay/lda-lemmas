\documentclass[11pt,letterpaper]{article}
\usepackage{rotating}
\usepackage{acl2016}
\usepackage{times}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{booktabs}
\usepackage[small]{caption}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{rotating}

\usepackage[inline,shortlabels]{enumitem}
\setlist*[enumerate,1]{label=$(\arabic*)$}
\setlist*[itemize,1]{label=$\bullet$}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage[russian,english]{babel}

\usepackage{cleveref}
\crefname{section}{§}{§§}
\Crefname{section}{§}{§§}
\crefname{figure}{Fig}{}
\crefname{algorithm}{Alg.}{}
\crefformat{section}{§#2#1#3}  % remove space between section symbol and the number


\long\def\devour#1{\ignorespaces}

\renewcommand{\vec}{\boldsymbol}   % optional
\newcommand{\vf}{{\vec{f}}}
\newcommand{\vg}{{\vec{g}}}
\newcommand{\vh}{{\vec{h}}}
\newcommand{\vx}{{\vec{x}}}
\newcommand{\vz}{{\vec{z}}}
\newcommand{\vm}{{\vec{m}}}
\newcommand{\vw}{{\vec{w}}}
\newcommand{\vc}{{\vec{c}}}
\newcommand{\vy}{{\vec{y}}}
\newcommand{\vl}{{\vec{l}}}
\newcommand{\vs}{{\vec{s}}}
\newcommand{\vphi}{{\vec{\phi}}}
\newcommand{\valpha}{{\vec{\alpha}}}
\newcommand{\vxi}{{\vec{\xi}}}

\newcommand{\vmu}{{\vec{\mu}}}
\newcommand{\vtheta}{{\vec{\theta}}}
\newcommand{\veta}{{\vec{\eta}}}
\newcommand{\vomega}{{\vec{\omega}}}
\newcommand{\vpsi}{{\vec{\psi}}}

\newcommand{\diracdelta}[2]{\delta_{#1}\left(#2\right)}

\newcommand{\DR}{\ensuremath{\textrm{DR}}}
\newcommand{\model}{\ensuremath{(model)}}
\newcommand{\DRmodel}{\ensuremath{\DR^{\model}}}
\newcommand{\DRlem}{\ensuremath{\DR^{(lem)}}}
\newcommand{\DRnonlem}{\ensuremath{\DR^{(non-lem)}}}

% reduce space before \paragraph command -- http://tex.stackexchange.com/questions/219676/how-to-change-spacing-before-and-after-paragraph-command/219677#219677
%\makeatletter
%\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
%                                    {3.25ex \@plus1ex \@minus.2ex}%
%                                    {-1em}%
%                                    {\normalfont\normalsize\bfseries}}
%\makeatother

\usepackage{amssymb}
\DeclareMathOperator*{\Expect}{\mathop{\mathbb{E}}}
\DeclareMathOperator*{\argmax}{\mathop{\text{argmax}}}


% new math commands

\usepackage{color}
\usepackage{xcolor}
\definecolor{darkgrey}{rgb}{0.2,0.2,0.2}
\definecolor{grey}{rgb}{0.9,0.9,0.9}
\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\definecolor{darkpurple}{rgb}{0.4,0.0,0.4}
\definecolor{darkred}{rgb}{0.5,0.0,0.0}
\definecolor{darkorange}{rgb}{0.5,0.45,0.4}
\definecolor{darkgreen}{rgb}{0.0,0.5,0.0}
\definecolor{darkergreen}{rgb}{0.0,0.4,0.0}
\definecolor{lightblue}{rgb}{0.8,0.8,1.0}
\definecolor{lightgreen}{rgb}{0.8,1.0,0.8}
\definecolor{lightred}{rgb}{1.0,0.8,0.8}
\definecolor{lightyellow}{rgb}{1.0,1.0,0.8}
\definecolor{lightorange}{rgb}{1.0,0.9,0.8}
\definecolor{lightgrey}{rgb}{0.96,0.97,0.98}
\definecolor{brilliantlavender}{rgb}{0.96, 0.73, 1.0}
\definecolor{deeppink}{rgb}{1.0,0.08,0.58}
\definecolor{hotpink}{rgb}{1.0,0.41,0.71}
\definecolor{pink}{rgb}{1.0,0.75,0.80}

% define notes
\usepackage[russian,english]{babel}
\usepackage{soul}
\usepackage{color}
\newcommand{\Note}[3]{\sethlcolor{#2}\hl{[\textbf{#1}: #3]}}
%\renewcommand{\Note}[3]{}

\newcommand{\timv}[1]{\Note{timv}{brilliantlavender}{#1}}
\newcommand{\ryan}[1]{\Note{ryan}{lightorange}{#1}}
\newcommand{\chandler}[1]{\Note{chandler}{pink}{#1}}
\newcommand{\todo}[1]{\Note{todo}{red}{#1}}



\newcommand{\defn}[1]{{\bf #1}}%

%i commented this out because i cannot compile this option
%what's the easiest way to install this on my mac?
%\usepackage[subtle]{savetrees}

\newcommand{\software}[1]{\textsc{#1}}
\newcommand{\word}[1]{{\em #1}}
\newcommand{\lemma}[1]{\textsc{#1}}
\newcommand{\att}[2]{{\small \texttt{#1}}=\textsc{#2}}
\newcommand{\gloss}[1]{``#1''}
\newcommand{\UNK}{\textsc{unk}\xspace}

%\newcommand{\func}{\mathcal{E}(t, s, u, w)}
%\newcommand{\grad}{\nabla_{\vtheta}\,\func}
%\newcommand{\funcw}{\mathcal{E}(t, s, u)}
%\newcommand{\gradw}{\nabla_{\vtheta}\,\funcw}

\def\figref#1{Figure~\ref{fig:#1}}
\def\figlabel#1{\label{fig:#1}\label{p:#1}}
\def\Tabref#1{Table~\ref{tab:#1}}
\def\tabref#1{Table~\ref{tab:#1}}
\def\tablabel#1{\label{tab:#1}\label{p:#1}}
\def\Secref#1{Section~\ref{sec:#1}}
\def\secref#1{Section~\ref{sec:#1}}
\def\seclabel#1{\label{sec:#1}\label{p:#1}}
\def\eqref#1{Eq.~\ref{eqn:#1}}
\def\eqrefn#1{\ref{eqn:#1}}
\def\eqsref#1#2{Eqs.~\ref{eqn:#1}-\ref{eqn:#2}}
\def\eqlabel#1{\label{eqn:#1}}
\def\subsp#1{P_{\mbox{{\scriptsize\rm #1}}}}



% space hacks
%\setlength\belowcaptionskip{-4pt}
%\setlength\titlebox{2cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

%\titlespacing\section{0pt}{1pt plus 4pt minus 4pt}{12pt plus 4pt minus 4pt}
%\titlespacing\subsection{0pt}{1pt plus 0pt minus 0pt}{1pt plus 0pt minus 0pt}
%\titlespacing\subsubsection{0pt}{1pt plus 0pt minus 0pt}{1pt plus 0pt minus 0pt}

\title{Topic Modeling for Morphologically Rich Languages}

\begin{document}
\maketitle
\begin{abstract}
\end{abstract}

\section{Introduction}\label{sec:introduction}

Topic modeling is a standard tool for unsupervised analysis of large
text corpora. At the core, almost all topic models pick up on
co-occurrence signals between different words in the corpus, that is,
words that occur often in the same sentence, are likely to belong to
the same latent topic. In languages that exhibit rich inflection
morphology, the signal becomes weaker given the proliferation of
unique tokens. In this work, we explore the effect of token-based
lemmatization on the performance of topic models.

Syntactic information is not generally considered to exert a strong
force on the thematic nature of a document.  Indeed, for this reason
topic models often make a bag-of-words assumption, the whereby exact
order of the words in a sentence is discard. In morphologically rich
languages, however, syntactic information is often encoded in the word
form itself. Nevertheless, most commonly employed approaches to topic
modeling ignore the influence of morphology. Consider the Russian name
{\em Putin}; in English, we have a single type that represents in the
concept in all syntactic contexts, whereas in Russian
{\selectlanguage{russian} Путин} appears with various inflections,
e.g., {\selectlanguage{russian}Путина},
{\selectlanguage{russian}Путину}, {\selectlanguage{russian}Путине},
and {\selectlanguage{russian}Путином}. Which form of the name one uses
is fully dependent on the syntactic structure of the sentence. Compare
the utterances {{\selectlanguage{russian}мы говорим о Путине} ({\em we
    are speaking about Putin}) and {{\selectlanguage{russian}мы
      говорим Путину} ({\em we are speaking to Putin})}, both sentence
  are thematically centered on Putin, but two different words forms
  are employed.

  \ryan{Make connection here to stop words. We filter stop words (or try to) in topic models, so we should
    fitler these endings as well?}
  
\section{Morphology and Lemmatization}\label{sec:inflectional}
Morphology concerns itself with the internal structure of individual
words.  Specifically, we focus on {\em inflectional morphology}, word
internal structure that marks syntactically relevant linguistic
properties, e.g., person, number, case and gender on the word form
itself. While inflectional morphology is minimal in English and
virtually non-existent in Chinese, it occupies a prevalent position in
many languages' grammars, e.g., Russian. In fact, Russian will often
express relations marked in English with prepositions, simply through
the addition of a suffix, often reducing the number of words in a
given sentence.  \ryan{Need to say more?}

In the context of NLP, heavily inflected inflected languages have an
increased token to type ratio, greatly increasing the number of
unknown owords. One method to combat is to {\em lemmatize} the sentence.
A lemmatizer maps each inflection of a word, to a canonicalized form. 


\begin{table}
  \begin{tabular}{l | l l }
    & {\bf Singular} & {\bf Plural} \\ \hline
    {\bf Nominative} &  {\selectlanguage{russian}пес} ({\em pyos}) & {\selectlanguage{russian}псы}    ({\em psy})   \\
    {\bf Genitive} &  {\selectlanguage{russian}пса} ({\em psa}) & {\selectlanguage{russian}псов}    ({\em psov})  \\
    {\bf Accusative} &  {\selectlanguage{russian}пса} ({\em psa}) & {\selectlanguage{russian}псов}    ({\em psov})  \\
    {\bf Dative} &  {\selectlanguage{russian}псу} ({\em psu}) & {\selectlanguage{russian}псам}    ({\em psam})  \\
    {\bf Locative} &  {\selectlanguage{russian}псе} ({\em psye}) & {\selectlanguage{russian}псах}   ({\em psax})  \\
    {\bf Instrumental} &  {\selectlanguage{russian}псом} ({\em psom}) & {\selectlanguage{russian}псами}  ({\em psami}) \\
  \end{tabular}
  \caption{A declension table for the Russian word
    {\selectlanguage{russian}пес} ({\em pyos}), meaning ``dog''.  Each
    of the 12 different entries in the table occurs in a distinct
    syntactic context. A lemmatizer canonicalizes these forms to
    single form, which is the nominative singular in the case of
    Russian, greatly reducing the sparsity present in the corpus.
    \todo{reference this in text}}
\end{table}

  

\section{Related Work}\label{sec:related-work}
\ryan{I can probably do the non-topic-modeling part of this section.}

Though applied in many
studies~\cite{deerwester1990,hofmann1999,mei2007,nallapati2008,lin2009},
lemmatization has not been directly explored in the context of topic
modeling.  An infinite-vocabulary LDA model containing a prior on words
similar to an n-gram model has been developed~\cite{zhai2013}; this
prior could be viewed as loosely encoding beliefs of a
concatenative morphology, but its effect was not analyzed in
isolation.


\section{Experiments}\label{sec:experiments}

To measure the effect of lemmatization on topic models we must first
define a topic model.  In this study, for comparability with other
work, we restrict our attention to latent Dirichlet allocation
(LDA)~\cite{blei2003}, the canonical Bayesian graphical topic model.
However, we want to measure the performance of a topic model by its
interpretability, as topic models are surpassed by other models,
including traditional vector space models, in tasks without a ``human
in the loop''~\cite{may2015}.  Thus we note there are more modern but
less widely-used topic models such as the sparse additive generative
(SAGE) topic model, which explicitly models the background word
distribution and encourages sparse topics~\cite{eisenstein2009}, or the
nested hierarchical Dirichlet process (nHDP) topic model, which
represents topics in a hierarchy and automatically infers its effective
size~\cite{paisley2015}.  These models may render more interpretable
results overall.  However, we are currently interested in the
\emph{relative} impact of lemmatization on a topic model, we are
unaware of any direct prior work, and we wish for our results to be
widely applicable across research and industry.  Thus we leave these
alternative topic models as considerations for future work.

\todo{LDA is defined as...}

We must also determine how to measure the quality of a
topic model.  In particular, in this study we require an evaluation
metric that is comparable across different views of the same corpus.
Additionally, we desire an evaluation metric that correlates with a
human's ability to use the model to explore or filter a large dataset,
hence, the interpretability of the model.  Thus we evaluate models in
this study by \emph{word intrusion} performance.  In the word intrusion
evaluation, a human expert is given one topic at a time, represented
by its top $m$ words (for some small number $m$) in random order, as
well as an additional word, called the \emph{intruder}, randomly placed
among the $m$ topic words~\cite{chang2009}.
The intruder is randomly selected from the set of high-probability
words from other topics in the model.
The expert is tasked with identifying the intruder in each list of
$m + 1$ words (where each list corresponds to a topic from the model).
As in prior work~\cite{chang2009}, we instruct the expert to ignore
syntactic and morphological patterns.  Examples from our word
intrusion interface on both the lemmatized and non-lemmatized data
are depicted in Figure~\ref{fig:word-intrusion}.

\begin{figure}
    \caption{\todo{word intrusion examples}}
    \label{fig:word-intrusion}
\end{figure}

If the model is interpretable, the $m$ words from a topic will be
internally coherent whereas the intruder word is likely to stand out.
Thus a model's interpretability can be quantified by the fraction
of topics for which the expert correctly idenfies the intruder.  We
call this value the \emph{detection rate}:
\begin{align*}
    \DR = \frac{1}{K} \sum_{k=1}^K \diracdelta{i_k}{\omega_k}
\end{align*}
where $K$ is the number of topics in the model, $i_k$ is the index
of the intruder in the randomized word list generated from topic $k$,
and $\omega_k$ is the index of the word the expert identified as the
intruder.  We note this is just the mean (over topics) of the
\emph{model precision} metric from prior work~\cite{chang2009}
when one expert is used instead of several non-experts.

\chandler{topic model quality correlates with size (we should be
          able to measure this)} \cite{mimno2011}

\todo{explain vocabulary filtering};
\todo{explain doc truncation}

We learn all models by stochastic variational
inference~\cite{hoffman2013}, initializing the models randomly and
using fixed priors.\footnote{In preliminary experiments Gibbs
sampling with hyper-parameter optimization was generally found to not
help, or even to hinder, interpretability.}
We specified $K = 100$ topics to all models.
Uniform priors with \todo{hyperparameters} were given to models on
filtered vocabularies; non-uniform priors with \todo{hyperparameters}
were given to models on unfiltered vocabularies.
\todo{give details of data collection, lemmatizer};
\todo{explain unknown backoff, give num/frac unknown};

The detection rate for all four configurations and the
p-values for one-sided detection rate differences (testing our
hypothesis that the lemmatized models are more interpretable than the
non-lemmatized models) are reported in
Table~\ref{tab:detection-rate}.  Word intrusion performance benefits
significantly from lemmatization on a filtered vocabulary.  On an
unfiltered vocabulary with a prior designed to reduce topic pollution
by stop words lemmatization renders no significant improvement.
Additionally, on length-truncated documents, neither the filtered nor
unfiltered models are benefitted by lemmatization.

\begin{table}
    \begin{tabular}{l|rr|r}
        experiment            & non-lem $\DR$ &     lem $\DR$ &       p-value \\\hline
        whole                 &          0.54 &          0.52 &          0.61 \\
        \textbf{whole + filt} & \textbf{0.50} & \textbf{0.65} & \textbf{0.02} \\
        trunc                 &          0.37 &          0.37 &          0.50 \\
        trunc + filt          &          0.43 &          0.47 &          0.28 \\
    \end{tabular}
    \caption{Detection rate ($\DR$) for the non-lemmatized and
        lemmatized models
        and p-values for the test that the lemmatized detection rate
        is greater than the non-lemmatized detection rate.
        Detection rate is significantly benefitted by lemmatization on
        a filtered vocabulary, highlighted in bold.}
    \label{tab:detection-rate}
\end{table}

\section{Discussion}\label{sec:discussion}


\section{Conclusion}\label{sec:conclusion}

\bibliographystyle{acl2016}
\bibliography{russian}

\end{document}
