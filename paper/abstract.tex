Topic models are typically represented by top-$m$ word lists for human
interpretation.  The corpus is often pre-processed with lemmatization
(or stemming)
so that those representations are not undermined by a proliferation of
words with similar meanings, but the effects of that pre-processing are
assumed rather than measured.  Recent work studied the effect of
stemming on topic models of English texts
and found no supporting evidence for this practice.  We study the
effect of lemmatization on topic models on Russian Wikipedia articles,
finding in one configuration that it significantly improves a measure
of interpretability.  We conclude that lemmatization may benefit
topic models on morphologically rich languages, and that further
investigation is needed.
