    Topic models make strong assumptions about their data.  In
    particular, different words are implicitly assumed to
    have different meanings: topic models are often used as
    human-interpretable dimensionality reductions and a proliferation
    of words with identical meanings would undermine the utility of the
    top-$m$ word list representation of a topic.  Though a number
    of authors have added pre-processing steps such as lemmatization to
    better accommodate these assumptions, the effects of such
    pre-processing procedures are rarely measured.  We test the
    effect of lemmatization on the interpretability of a
    latent Dirichlet allocation (LDA) model on Russian Wikipedia
    articles.  Using a word intrusion evaluation, we quantitatively
    demonstrate that lemmatization provides a significant benefit to
    the interpretability of a model in this corpus of morphologically
    rich text.
